{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Array Max :  [3. 9.]\n",
      "[[0.66666667 1.        ]\n",
      " [0.33333333 0.55555556]\n",
      " [1.         0.66666667]]\n",
      "[[0.92]\n",
      " [0.89]\n",
      " [0.86]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "x = np.array(([2,9],[1,5],[3,6]),dtype=float) #Hours studied,Hours slept\n",
    "y = np.array(([92],[89],[86]),dtype=float) #Marks scored\n",
    "c=np.amax(x,axis=0)\n",
    "print(\"Array Max : \",c)\n",
    "x=x/c\n",
    "y=y/100 #Test is out of 100 Marks\n",
    "print(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x): #Sigmoid Function #this function maps any value between 0 and 1\n",
    "    return 1/(1 + np.exp(-x))\n",
    "def sigmoid_grad(x): #Derivative of Sigmoid Function\n",
    "    return x*(1-x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Variable initialization\n",
    "epoch = 1000 #Setting training iterations\n",
    "eta = 0.1 #Setting learning Rate\n",
    "input_neurons = 2 #number of features in dataset\n",
    "hidden_neurons = 3 #number of hidden layer neurons\n",
    "output_neurons = 1 #number of output layer neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.70038302 0.32939857 0.75290751]\n",
      " [0.68158537 0.44152593 0.179174  ]]\n",
      "[[0.75819999 0.4546805  0.89344409]]\n",
      "[[0.66268422]\n",
      " [0.76598604]\n",
      " [0.42394659]]\n",
      "[[0.21530766]]\n"
     ]
    }
   ],
   "source": [
    "#Weight and Bias initializations\n",
    "wh = np.random.uniform(size=(input_neurons,hidden_neurons)) #2,3\n",
    "print(wh)\n",
    "bh = np.random.uniform(size=(1,hidden_neurons)) #1,3\n",
    "print(bh) #bias matrix to the hidden layer\n",
    "wout = np.random.uniform(size=(hidden_neurons,output_neurons)) #3,1\n",
    "print(wout) #weight matrix to output layer\n",
    "bout = np.random.uniform(size=(1,output_neurons)) #1,1\n",
    "print(bout) #Bias Matrix to the output layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(epoch):\n",
    "    #Forward Propagation\n",
    "    h_ip = np.dot(x,wh)+bh #Bias Hidden Gradient Descent\n",
    "    #print(h_ip)\n",
    "    h_act = sigmoid(h_ip) #Hidden layer activation\n",
    "    o_ip = np.dot(h_act,wout)+bout\n",
    "    #print(o_ip)\n",
    "    output = sigmoid(o_ip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Delta(Change Factor) at output layer is  [[0.00924751]\n",
      " [0.00803037]\n",
      " [0.00141787]]\n"
     ]
    }
   ],
   "source": [
    "#Backward Propagation\n",
    "#error at output layer\n",
    "Eo = y-output #compare prediction with actual output and calculate the gradient of error(Actual - Predicted)\n",
    "outgrad = sigmoid_grad(output) #Compute the slope/ gradient of hidden and output layer neurons\n",
    "d_output = Eo*outgrad #Compute change factor(delta) at output layer, dependent on the gradient of error multiplied by the slope of output layer activation\n",
    "print(\"Delta(Change Factor) at output layer is \",d_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#error at hidden layer\n",
    "Eh = d_output.dot(wout.T) #At this step, the error will propagate back into the network which means error at hidden layer. we will take the dot product of output layer delta with weight parameters of edges between the hidden and output layer (wout.T).\n",
    "hiddengrad = sigmoid_grad(h_act) #how much hidden layer weight contributed to error\n",
    "d_hidden = Eh*hiddengrad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "wout += h_act.T.dot(d_output)*eta\n",
    "wh += x.T.dot(d_hidden)*eta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized input : \n",
      " [[0.66666667 1.        ]\n",
      " [0.33333333 0.55555556]\n",
      " [1.         0.66666667]]\n",
      "Actual output : \n",
      " [[0.92]\n",
      " [0.89]\n",
      " [0.86]]\n",
      "Predicted output : \n",
      " [[0.84818449]\n",
      " [0.83243047]\n",
      " [0.84894346]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Normalized input : \\n\",str(x))\n",
    "print(\"Actual output : \\n\",str(y))\n",
    "print(\"Predicted output : \\n\",output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
